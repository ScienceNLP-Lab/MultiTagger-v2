{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3caaf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e2ab6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "year = \"2023\" \n",
    "current_directory = os.getcwd() + \"/data_summarization_with_abstract/\" + year + \"/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "082baf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mengfeilan/Documents/multitagger/data_summarization_with_abstract/2023/'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9c96841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        html_doc = ''.join(f.readlines())\n",
    "    return html_doc\n",
    "\n",
    "\n",
    "def sentence_extract(p, full_text):\n",
    "    p_text = ''\n",
    "    for n in p.contents:\n",
    "        if n.name == None:\n",
    "            p_text += n.replace('\\n', ' ')\n",
    "        elif n.name in ['xref', 'sup', 'italic']:\n",
    "            p_text += n.text.replace('\\n', ' ')\n",
    "        elif n.name in ['table-wrap', 'fig']:\n",
    "            # title of figure and table\n",
    "            label = n.label.text.replace('\\n', ' ') if n.label else ''\n",
    "            caption = n.caption.text.replace('\\n', ' ') if n.caption else ''\n",
    "            caption_text = label + \" \" + caption + \". \"\n",
    "            full_text += caption_text\n",
    "        elif n.name == \"title\":\n",
    "            print(\"title: \", n.get_text())\n",
    "            p_text += n.get_text() + \"\\n\"\n",
    "            \n",
    "        elif n.name == \"ext-link\":\n",
    "            p_text += n.get_text()\n",
    "        else:\n",
    "            n_text = '. '.join([i.get_text() for i in n.findAll('p')])\n",
    "            p_text += n_text + \" \"\n",
    "    full_text += p_text + '\\n'\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def paragraph_extract(sec, full_text, subsection_mark):\n",
    "    if sec.find('p', recursive=False):\n",
    "        for p in sec.findAll('p', recursive=False):\n",
    "            full_text = sentence_extract(p, full_text)\n",
    "        if subsection_mark == 0:\n",
    "            full_text += '\\n'\n",
    "#         if subsection_mark == 1:\n",
    "#             full_text += '\\n'\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def iter_extract(sec, full_text, text_section, subsection_mark):\n",
    "    if sec.find('sec', recursive=False):\n",
    "        section_level = 0\n",
    "        pre_c = len(full_text)\n",
    "        if sec.title:\n",
    "            full_text += sec.title.text + '\\n'\n",
    "#         if sec.find('fig'):\n",
    "#             for fig in sec.findAll('fig', recursive=False):\n",
    "#                 label = fig.label.text.replace('\\n', ' ') if fig.label else ''\n",
    "#                 caption = fig.caption.text.replace('\\n', ' ') if fig.caption else ''\n",
    "#                 caption_text = label + \" \" + caption + \". \"\n",
    "#                 full_text += caption_text + \"\\n\"\n",
    "        \n",
    "        if len(sec.findAll('sec', recursive=False)) != 0:\n",
    "            section_level += 1\n",
    "            subsection_mark += 1\n",
    "            full_text = paragraph_extract(sec, full_text, subsection_mark)\n",
    "\n",
    "        for i in sec.findAll('sec', recursive=False):\n",
    "            full_text, text_section = iter_extract(i, full_text, text_section, subsection_mark)\n",
    "\n",
    "        after_c = len(full_text)\n",
    "        sec_name = sec.title.text if sec.title else ''\n",
    "        if sec_name in text_section.keys():\n",
    "            if text_section[sec_name][-1]-pre_c == -1:\n",
    "                text_section[sec_name][-1] = after_c-1\n",
    "                text_section[sec_name][0] = section_level\n",
    "            elif text_section[sec_name][-2] == pre_c and text_section[sec_name][-1] == after_c-1:\n",
    "                pass\n",
    "            else:\n",
    "                text_section[sec_name].extend([pre_c, after_c-1,section_level])\n",
    "        else:\n",
    "            text_section[sec.title.text if sec.title else ''] = [pre_c, after_c-1,section_level]\n",
    "\n",
    "        full_text += '\\n'\n",
    "        return full_text, text_section\n",
    "    \n",
    "    elif not sec.find('p', recursive=False):\n",
    "        section_level = 0\n",
    "        if sec:\n",
    "            pre_c = len(full_text)\n",
    "            full_text = paragraph_extract(sec, full_text, subsection_mark) #change here\n",
    "            full_text += '\\n'\n",
    "            after_c = len(full_text)\n",
    "            sec_name = sec.title.text if sec.title else ''\n",
    "            if sec_name in text_section.keys():\n",
    "                if text_section[sec_name][-1]-pre_c == -1:\n",
    "                    text_section[sec_name][-1] = after_c-1\n",
    "                    text_section[sec_name][0] = section_level\n",
    "                else:\n",
    "                    text_section[sec_name].extend([pre_c, after_c-1,section_level])\n",
    "            else:\n",
    "                text_section[sec.title.text if sec.title else ''] = [pre_c, after_c-1,section_level]\n",
    "            return full_text, text_section\n",
    "    else:\n",
    "        section_level = 0\n",
    "        pre_c = len(full_text)\n",
    "        if sec.title:\n",
    "            full_text += sec.title.text.replace('\\n', ' ') + '\\n'\n",
    "        full_text = paragraph_extract(sec, full_text, subsection_mark)\n",
    "        # full_text += ' '.join([j.get_text().replace('\\n', ' ') for j in sec.findAll('p')]) + '\\n'\n",
    "        after_c = len(full_text)\n",
    "        sec_name = sec.title.text if sec.title else ''\n",
    "        if sec_name in text_section.keys():\n",
    "            if text_section[sec_name][-1]-pre_c == -1:\n",
    "                text_section[sec_name][-1] = after_c-1\n",
    "                text_section[sec_name][0] = section_level\n",
    "            else:\n",
    "                text_section[sec_name].extend([pre_c, after_c-1,section_level])\n",
    "        else:\n",
    "            text_section[sec.title.text if sec.title else ''] = [pre_c, after_c-1,section_level]\n",
    "        return full_text, text_section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c624aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file, num=None):\n",
    "    subsection_mark = 0\n",
    "    html_doc = load_data(file)\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser').findAll('article', recursive=False)[0]\n",
    "\n",
    "#     if soup['article-type'] != 'research-article':\n",
    "#         return False, False, False\n",
    "\n",
    "    for c in soup.children:\n",
    "        if c.name not in ['front', 'body', 'back']:\n",
    "            c.replaceWith('')\n",
    "\n",
    "    section_header = []\n",
    "    text_section = dict()\n",
    "    article_meta = soup.findAll('article-meta')[0].findAll('article-id')\n",
    "\n",
    "    article_meta = [i.text for i in article_meta if i['pub-id-type'] == 'pmid'][0]\n",
    "#     if num:\n",
    "#         if article_meta in num:\n",
    "#             return False, False, False\n",
    "\n",
    "    full_text = ''\n",
    "    # title section\n",
    "\n",
    "    title = soup.findAll('title-group')\n",
    "    if len(title) == 1:\n",
    "        title = title[0].find('article-title').get_text(separator=' ').replace('\\n', ' ')\n",
    "        full_text += title+'\\n'\n",
    "    else:\n",
    "        # print(file, ' more than one title')\n",
    "        text_section['title']=[0, full_text.__len__()-1]\n",
    "\n",
    "    # abstract section\n",
    "    abstract = soup.findAll('abstract')\n",
    "    abstract_content = \"\"\n",
    "    if abstract:\n",
    "        for abs in abstract:\n",
    "            if abs.get('abstract-type') not in ['toc', 'graphical', 'teaser', 'author-highlights']:\n",
    "                if not abs.find('title', recursive=False):\n",
    "                    full_text += '\\n'\n",
    "                    \n",
    "                abs_sec = abs.contents\n",
    "                for sec in abs_sec:\n",
    "                    if type(sec) is bs4.element.NavigableString:\n",
    "                        full_text += sec\n",
    "                        abstract_content += sec\n",
    "                    elif type(sec) is bs4.element.Comment:\n",
    "                        pass\n",
    "                    elif type(sec) is bs4.element.Tag:\n",
    "                        if sec.title and sec.p:\n",
    "#                             full_text += sec.title.text\n",
    "#                             full_text += \"\\n\"\n",
    "#                             full_text += sec.p.text\n",
    "#                             full_text += \"\\n\"\n",
    "                            abstract_content += sec.title.text\n",
    "                            abstract_content += \"\\n\"\n",
    "                            abstract_content += sec.p.text\n",
    "                            abstract_content += \"\\n\"\n",
    "                        else:\n",
    "#                             full_text += sec.text\n",
    "#                             full_text += '\\n'\n",
    "                            abstract_content += sec.text\n",
    "                            abstract_content += '\\n'\n",
    "                    else:\n",
    "                        abstract_content, text_section = iter_extract(sec, \"\", text_section, subsection_mark)\n",
    "\n",
    "    else:\n",
    "        abstract = ''\n",
    "        try:\n",
    "            text_section['abstract'] = [text_section['title'][1], full_text.__len__()-1]\n",
    "        except:\n",
    "            return False, False, False, False\n",
    "\n",
    "    # body section\n",
    "    body = soup.findAll('body')\n",
    "    if len(body) > 0:\n",
    "        full_text += '\\n'\n",
    "        if len(body) == 1:\n",
    "            body = body[0].contents\n",
    "            for sec in body:\n",
    "                if type(sec) is bs4.element.NavigableString:\n",
    "                    full_text += sec\n",
    "                else:\n",
    "                    full_text, text_section = iter_extract(sec, full_text, text_section, subsection_mark)\n",
    "\n",
    "    # back section\n",
    "    back_matter_text = \"\"\n",
    "    back = soup.findAll('back')\n",
    "    if len(back) > 0:\n",
    "        for sec in back[0].contents:\n",
    "            #foot note\n",
    "            if sec.name == \"fn-group\":\n",
    "                foot_notes = sec.findAll(\"fn\")\n",
    "                for foot_note in foot_notes:\n",
    "                    foot_note_text = '\\n'.join([i.get_text() for i in foot_note.findAll('p')])  \n",
    "                    back_matter_text += foot_note_text\n",
    "                back_matter_text += \"\\n\"                      \n",
    "            #acknowledgement\n",
    "            if sec.name == \"ack\":\n",
    "                try:\n",
    "                    sec_title = sec.find(\"title\").get_text()\n",
    "                except:\n",
    "                    sec_title = \"\"\n",
    "                back_matter_text += sec_title\n",
    "                back_matter_text += \"\\n\"\n",
    "                sec_text = ' '.join([i.get_text() for i in sec.findAll('p', recursive=False)])          \n",
    "                back_matter_text += sec_text\n",
    "                sub_sections = sec.findAll('sec')\n",
    "                for sub_sec in sub_sections:\n",
    "                    if sub_sec.find(\"title\"):\n",
    "                        back_matter_text += \"\\n\"\n",
    "                        sub_sec_title = sub_sec.find(\"title\").get_text()\n",
    "                        back_matter_text += sub_sec_title\n",
    "                        back_matter_text += \"\\n\"\n",
    "                        sub_sec_text = ' '.join([i.get_text() for i in sub_sec.findAll('p')])\n",
    "                        back_matter_text += sub_sec_text\n",
    "                        \n",
    "            #notes\n",
    "            if sec.name == \"notes\":\n",
    "                sub_sections = sec.findAll('sec')\n",
    "                for sub_sec in sub_sections:\n",
    "                    if sub_sec.find(\"title\"):\n",
    "                        back_matter_text += \"\\n\" \n",
    "                        sub_sec_title = sub_sec.find(\"title\").get_text()\n",
    "                        back_matter_text += sub_sec_title\n",
    "                        back_matter_text += \"\\n\"\n",
    "                        sub_sec_text = ' '.join([i.get_text() for i in sub_sec.findAll('p')])\n",
    "                        back_matter_text += sub_sec_text\n",
    "            \n",
    "            #sections\n",
    "            if sec.name == \"sec\":\n",
    "                if sec.find(\"title\"):\n",
    "                    sec_title = sec.find(\"title\").get_text()\n",
    "                    back_matter_text += sec_title\n",
    "                    back_matter_text += \"\\n\"\n",
    "                    sec_text = ' '.join([i.get_text() for i in sec.findAll('p')])\n",
    "                    back_matter_text += sec_text\n",
    "                    \n",
    "            #glossary\n",
    "            if sec.name == \"glossary\":\n",
    "                if sec.find(\"title\"):\n",
    "                    sec_title = sec.find(\"title\").get_text()\n",
    "                    back_matter_text += sec_title\n",
    "                    terms = sec.findAll(\"def-item\")\n",
    "                    for term in terms:\n",
    "                        back_matter_text += \"\\n\"\n",
    "                        term_abb = term.find(\"term\").get_text()\n",
    "                        term_meaning = term.find(\"p\").get_text()\n",
    "                        back_matter_text += term_abb + \" \" + term_meaning\n",
    "                \n",
    "    full_text +=back_matter_text\n",
    "            \n",
    "    return abstract_content, full_text, article_meta, text_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8244ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brat_data(input_folder,output_folder):   \n",
    "    annotation = \"\"\n",
    "    processed_full_text = \"\"\n",
    "    files_list = os.listdir(input_folder)\n",
    "    for PMCID in files_list:\n",
    "        if PMCID.startswith('.'):\n",
    "            continue\n",
    "        folder_path = input_folder + PMCID + \"/\"\n",
    "        files = os.listdir(folder_path)\n",
    "        for file in files:\n",
    "\n",
    "            if \".nxml\" in file:\n",
    "                file_path = folder_path + file\n",
    "                abstract_content, full_text, article_meta, text_section = extract_data(file_path)\n",
    "                if full_text == None:\n",
    "                    continue\n",
    "                text_file_name_full = output_folder + str(PMCID) + \"_fulltext.txt\"\n",
    "                text_file_name_abs = output_folder + str(PMCID) + \"_abs.txt\"\n",
    "                textfile_full = open(text_file_name_full,\"w\") \n",
    "                textfile_abs = open(text_file_name_abs, \"w\")\n",
    "                if full_text == False:\n",
    "                    continue\n",
    "                if abstract_content == False:\n",
    "                    continue\n",
    "                textfile_full.write(full_text) \n",
    "                textfile_abs.write(abstract_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7950ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mengfeilan/Documents/multitagger/data_summarization_with_abstract/2023/NXML_second/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [90], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_folder)\n\u001b[1;32m      3\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m current_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfulltext_and_abstract/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m generate_brat_data(input_folder,output_folder)\n",
      "Cell \u001b[0;32mIn [89], line 14\u001b[0m, in \u001b[0;36mgenerate_brat_data\u001b[0;34m(input_folder, output_folder)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nxml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     13\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m folder_path \u001b[38;5;241m+\u001b[39m file\n\u001b[0;32m---> 14\u001b[0m     abstract_content, full_text, article_meta, text_section \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m full_text \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [88], line 148\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(file, num)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     back_matter_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m                     term_abb \u001b[38;5;241m=\u001b[39m term\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[0;32m--> 148\u001b[0m                     term_meaning \u001b[38;5;241m=\u001b[39m \u001b[43mterm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\n\u001b[1;32m    149\u001b[0m                     back_matter_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m term_abb \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m term_meaning\n\u001b[1;32m    151\u001b[0m full_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mback_matter_text\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "input_folder = current_directory + \"NXML_second/\"\n",
    "print(input_folder)\n",
    "output_folder = current_directory + \"fulltext_and_abstract/\"\n",
    "generate_brat_data(input_folder,output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a9f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ddd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
